{
  "hash": "762f4615d801f23e910c1d438d350d63",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"I am 95% confident that I don't know what a confidence interval is\"\nauthor: \"Louie Neri\"\ndate: \"06/26/2024\"\ndraft: true\nformat:\n  html:\n    toc: true\n    mermaid:\n      theme: default\n    code-fold: true\n---\n\n\nConfidence intervals (CI) have been the bane of my scientific existence. They are very unintuitive, and even statisticians [debate on how to best describe a confidence interval](https://www.bmj.com/content/366/bmj.l5381).\n\nLet's dive into the depths of my mind as I try to figure out what the heck a confidence interval is.\n\n# Confident about... what?\n\nThe more specific definition of a 95% confidence interval (using 95% for the sake of convention) is as follows:\n\n> If I were to repeat this sampling process over and over, 95% of the confidence intervals would contain the true value.\n\nNow, here's where the confusion lies. Intuitively, this would lead you to think that it is the probability that the true value is in that interval. NOPE!\n\nLet's look at it another way. Imagine we're trying to determine the mean height of all NBA players (yes, another basketball reference). Here's how it might play out:\n\n-   You get a random sample of players, calculate the mean height, and determine a 95% confidence interval.\n\n-   I decide to do the same thing. I get my own random sample, calculate the mean, and find my own 95% confidence interval.\n\n-   Then our friend Alexis joins in. She gets her random sample and calculates her mean and 95% confidence interval.\n\nIf we (and others) kept doing this over and over, about 95% of all these confidence intervals would contain the true mean height of NBA players. However, for any single interval – yours, mine, or Alexis's – we can't know for certain whether it's one of the 95% that contains the true mean or one of the 5% that doesn't.\n\nI don't think it would be very useful to demonstrate this idea in much detail, since [Kristoffer Magmnusson's visualization is more than sufficient](https://rpsychologist.com/d3/ci/) and I would recommend playing around with it!\n\n[![Courtsey of Kristoffer Magmusson](images/clipboard-4120711539.png){width=\"335\"}](https://rpsychologist.com/d3/ci/)\n\n# Interpreting single confidence intervals\n\nI think it's more useful to think about how to interpret *single* confidence intervals. Since, a majority of the time, we're reading studies that just spit out one confidence interval.\n\nWhen looking at a confidence interval, it's best to think of it as a measure of how *precise* the estimate is.\n\nThe width of the confidence interval will tell you how close you are to the true estimate. Note, this does NOT tell you how accurate the estimate is.\n\nLet's simulate a meta-analysis for the sake of example.\n\nIn this example, we are looking at the effect of my new teaching method called the Learning Optimization and Understanding through Interactive Engagement (LOUIE) Method, compared to traditional teaching methods, on math scores of high school students. Each study represents a different school studying the LOUIE method and comparing it to traditional teaching, and getting the mean test scores for each group.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set seed for reproducibility\nset.seed(1234)\n\n#load packages\nlibrary(\"metafor\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: metadat\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: numDeriv\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nLoading the 'metafor' package (version 4.6-0). For an\nintroduction to the package please type: help(metafor)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Number of studies\nn <- 5\n\n# Simulate data\nsim_data <- data.frame(\n  study = paste(\"Study\", 1:n),\n  author = c(\"Smith et al.\", \"Johnson et al.\", \"Williams et al.\", \"Brown et al.\", \"Davis et al.\"),\n  year = c(2018, 2019, 2020, 2021, 2022),\n  n1i = c(100, 150, 200, 120, 250),  # sample size group 1\n  n2i = c(110, 140, 190, 130, 250),  # sample size group 2\n  m1i = c(9.2, 9.5, 9.8, 10.2, 8.4),  # mean of group 1\n  m2i = c(9.0, 9.8, 10.3, 10.9, 8.5),  # mean of group 2\n  sd1i = c(2.1, 1.9, 2.3, 2.0, 2.2),  # standard deviation of group 1\n  sd2i = c(2.2, 2.0, 2.1, 2.3, 2.1)   # standard deviation of group 2\n)\n\n#calcuate SMD\ndat <- escalc(measure = \"SMD\",\n                       m1i = m1i,\n                       m2i = m2i,\n                       sd1i = sd1i,\n                       sd2i = sd2i,\n                       n1i = n1i,\n                       n2i = n2i,\n                       data = sim_data)\n\n#view\ndat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    study          author year n1i n2i  m1i  m2i sd1i sd2i      yi     vi \n1 Study 1    Smith et al. 2018 100 110  9.2  9.0  2.1  2.2  0.0926 0.0191 \n2 Study 2  Johnson et al. 2019 150 140  9.5  9.8  1.9  2.0 -0.1535 0.0139 \n3 Study 3 Williams et al. 2020 200 190  9.8 10.3  2.3  2.1 -0.2263 0.0103 \n4 Study 4    Brown et al. 2021 120 130 10.2 10.9  2.0  2.3 -0.3229 0.0162 \n5 Study 5    Davis et al. 2022 250 250  8.4  8.5  2.2  2.1 -0.0464 0.0080 \n```\n\n\n:::\n\n```{.r .cell-code}\n#random effects model\nmodel_result <- rma(yi, vi, data = dat)\n\n#create forest plot\nforest(model_result, header = TRUE, xlab = \"LOUIE vs Traditional\")\n```\n\n::: {.cell-output-display}\n![](confidenceintervals_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nIn the meta-analysis above, we're looking at the standardized mean differences (SMDs) of 5 studies, and their 95% confidence intervals. As we can see, maybe there's something to the LOUIE method? All this talk about my teaching method is really a distraction, let's look at the confidence intervals.\n\nFor the sake of brevity, I will not go into the math behind all of this. But in general, as the sample size increases, the confidence interval will get narrower, and thus, the estimate becomes a bit more precise.\n\nLet's play around and increase the sample size of both groups in study 5 from 250 to 500. Take a look at what happens to the confidence interval of that study.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set seed for reproducibility\nset.seed(1234)\n\n#load packages\nlibrary(\"metafor\")\n\n# Number of studies\nn <- 5\n\n# Simulate data\nsim_data <- data.frame(\n  study = paste(\"Study\", 1:n),\n  author = c(\"Smith et al.\", \"Johnson et al.\", \"Williams et al.\", \"Brown et al.\", \"Davis et al.\"),\n  year = c(2018, 2019, 2020, 2021, 2022),\n  n1i = c(100, 150, 200, 120, 500),  # sample size group 1\n  n2i = c(110, 140, 190, 130, 500),  # sample size group 2\n  m1i = c(9.2, 9.5, 9.8, 10.2, 8.4),  # mean of group 1\n  m2i = c(9.0, 9.8, 10.3, 10.9, 8.5),  # mean of group 2\n  sd1i = c(2.1, 1.9, 2.3, 2.0, 2.2),  # standard deviation of group 1\n  sd2i = c(2.2, 2.0, 2.1, 2.3, 2.1)   # standard deviation of group 2\n)\n\n#calcuate SMD\ndat <- escalc(measure = \"SMD\",\n                       m1i = m1i,\n                       m2i = m2i,\n                       sd1i = sd1i,\n                       sd2i = sd2i,\n                       n1i = n1i,\n                       n2i = n2i,\n                       data = sim_data)\n\n#view\ndat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    study          author year n1i n2i  m1i  m2i sd1i sd2i      yi     vi \n1 Study 1    Smith et al. 2018 100 110  9.2  9.0  2.1  2.2  0.0926 0.0191 \n2 Study 2  Johnson et al. 2019 150 140  9.5  9.8  1.9  2.0 -0.1535 0.0139 \n3 Study 3 Williams et al. 2020 200 190  9.8 10.3  2.3  2.1 -0.2263 0.0103 \n4 Study 4    Brown et al. 2021 120 130 10.2 10.9  2.0  2.3 -0.3229 0.0162 \n5 Study 5    Davis et al. 2022 500 500  8.4  8.5  2.2  2.1 -0.0465 0.0040 \n```\n\n\n:::\n\n```{.r .cell-code}\n#random effects model\nmodel_result <- rma(yi, vi, data = dat)\n\n#create forest plot\nforest(model_result, header = TRUE, xlab = \"LOUIE vs Traditional\")\n```\n\n::: {.cell-output-display}\n![](confidenceintervals_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nNotice, it's a bit more narrow. This is because, we've increased the sample size, thus increasing the precision of the estimate.\n\nTo get back to what I said earlier, this does NOT tell us how accurate this estimate is. Since we have no idea whether the studies have other sources of bias, like sampling bias, allocation bias, or measurement bias, that may affect the estimate. Increasing the sample size has increased our \"confidence\" about the true estimate, and that in the long run, if we do this over and over, 95% of the time, it will contain the true estimate.\n\n# Do you feel confident?\n\nOverall, confidence intervals don't make sense. But, they are useful to give us an idea of how precise an estimate is and their behavior in the long-run across identical studies. If you really want to get the probability that the true parameter is in the interval, then look into the [Bayesian credible interval](https://en.wikipedia.org/wiki/Credible_interval#:~:text=credible%20intervals%20are%20intervals%20whose,not%20the%20object%20of%20probability.). But I can save that for another time.\n",
    "supporting": [
      "confidenceintervals_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}