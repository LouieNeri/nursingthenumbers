{
  "hash": "cfbb3d551a791564d86369f6730fc17a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"T-tests are just linear models\"\nauthor: \"Louie Neri\"\ndate: \"06/22/2024\"\nformat:\n  html:\n    toc: true\n    mermaid:\n      theme: neutral\n    code-fold: true\n---\n\n\n# The madness of statistical tests\n\nIn your introductory statistics class, you're introduced to a myriad of statistical tests that are confusing and have weird names of scientists you've never heard of. You do some Googling or look in the appendix of your textbook, and you find a neat graph that looks something like this:\n\n\n```{mermaid}\nflowchart TD\n  subgraph Parametric_assumptions[Parametric assumptions]\n        direction TB\n        Z1[1. Independent samples]\n        Z2[2. Data normally distributed]\n        Z3[3. Equal variances]\n    end\n\n    A[Type of data?] -->|Continuous| B[Type of question?]\n    A -->|Discrete, categorical| C[Any counts < 5?]\n    \n    B -->|Relationships| D[Do you have dependent & independent variables?]\n    B -->|Differences| E[Differences between what?]\n    \n    C -->|No| F[Chi-square tests, one and two sample]\n    C -->|Yes| G[Fisher's exact test]\n    \n    D -->|Yes| H[Regression analysis]\n    D -->|No| I[Correlation analysis]\n    \n    I -->|Parametric| J[Pearson's r]\n    I -->|Nonparametric| K[Spearman's rank correlation]\n    \n    E -->|Means| L[One-sample t-test]\n    E -->|Variances| M[Fmax test or Bartlett's test]\n    E -->|Multiple means Single variable| N[How many groups?]\n    \n    N -->|Two| O[Parametric assumptions satisfied?]\n    N -->|More than two| P[Parametric assumptions satisfied?]\n    \n    O -->|Yes| Q[Student's t-test]\n    O -->|No| R[Transform data?]\n    R -->|OK| Q\n    R -->|No| S[Mann-Whitney U or Wilcoxon test]\n    \n    P -->|Yes| T[One-way ANOVA]\n    P -->|No| U[Transform data?]\n    U -->|No| V[Kruskall-Wallis test]\n    U -->|OK| T\n    \n    T & V --> W[If significant, do post hoc test: Bonferroni's, Dunn's, Tukey's, etc.]\n    \n    \n```\n\n\nGreat! Now, you know exactly which test you need to do *and* when to do it. However, this is probably not the best statistical practice. For some if not most people, this will probably get the job done. But, you may find yourself in scenarios with messy data that do not fit neatly into the flow chart, leading you to inadvertently violate some crucial assumption of the statistical test, and your wonderfully designed study goes in the trash.\n\nLet me introduce you to the linear model! Believe it or not, most statistical tests are really just variations of the linear model. To understand this, let's simulate some data and see how a simple two sample t-test is really a linear model!\n\n# Testing the world of shoe color and dunking\n\n\n::: {.cell}\n\n:::\n\n\nHere's the scenario: we live in a hypothetical world where shoe color affects the vertical of basketball players. In this case, red shoes allow players to jump higher when compared to blue shoes. Since I cannot dunk a basketball (I'm 5'7\") and would love for this to be true, I'm going to put this to the test.\n\nI've recruited 40 players to study this shoe, allocating 20 players to wear the blue shoes and 20 to wear the red shoes. Each player records a vertical jump test, and their best height is recorded. Let's simulate this data and create a box plot to see how it looks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#setting the parameters\nset.seed(1234)\n\nn_players <- 40\nblue_mean <- 28\nred_mean <- 30\nsd <- 3\n\n#simulate jump heights in inches\nblue_jump_height <- rnorm(n_players, blue_mean, sd)\nred_jump_height <- rnorm(n_players, red_mean, sd)\n\n#create a dataframe\njump_data <- data.frame(\n  player_id = 1:(2 * n_players),\n  shoe_color = rep(c(\"Blue\", \"Red\"), each = n_players),\n  jump_height = c(blue_jump_height, red_jump_height)\n)\n\n#plot the data\nggplot(jump_data, aes(x = shoe_color, y = jump_height, color = shoe_color)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) + \n  geom_jitter() +\n  scale_color_manual(values = c(Blue = \"#00BFC4\", Red = \"#F8766D\")) +\n  labs(\n    x = \"Shoe color\",\n    y = \"Vertical jump (inches)\",\n  ) +\n  theme_tufte() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](ttestsarelinearmodels_files/figure-html/Simulating and visualizing data-1.png){width=672}\n:::\n:::\n\n\nIntuitively, we can tell that the blue shoes indeed increase vertical jump. However, let's verify this with the good ole two sample t-test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(jump_height ~ shoe_color, data = jump_data, var.equal = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  jump_height by shoe_color\nt = -4.4733, df = 78, p-value = 2.586e-05\nalternative hypothesis: true difference in means between group Blue and group Red is not equal to 0\n95 percent confidence interval:\n -4.343579 -1.668066\nsample estimates:\nmean in group Blue  mean in group Red \n          26.75840           29.76422 \n```\n\n\n:::\n:::\n\n\nNow, another way to test this is to view the problem in the context of a simple linear regression, with a binary predictor. In this case, the binary predictor would be blue shoes or red shoes. First, let's review the formula for a simple linear regression.\n\n$$\n \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x\n$$\n\nIf you're a bit rusty:\n\n-   $\\hat{y}$ represents is the estimated dependent variable, in this case, vertical jump height dependent on $x$ (in this case, shoe color).\n\n-   $\\hat{\\beta}_0$ represents the $\\hat{y}$ intercept when x = 0, but in this case, it would be for those who are in the blue shoe group.\n\n-   $\\hat{\\beta}_1x$ represents the estimated slope coefficient, which tell us that how much $y$ (jump height) changes, on average, for a one-unit increase in $x$.\n\nWhat you'll see in a moment, is that fitting a regression line using this binary predictor of shoe color, is the same as doing a two sample t-test.\n\nLet's see what happens when we compute the linear regression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#the regression model\nlm(formula = jump_height ~ shoe_color, data = jump_data) |> \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = jump_height ~ shoe_color, data = jump_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.795 -1.972 -0.425  1.714  8.489 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    26.7584     0.4751  56.317  < 2e-16 ***\nshoe_colorRed   3.0058     0.6720   4.473 2.59e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.005 on 78 degrees of freedom\nMultiple R-squared:  0.2042,\tAdjusted R-squared:  0.194 \nF-statistic: 20.01 on 1 and 78 DF,  p-value: 2.586e-05\n```\n\n\n:::\n\n```{.r .cell-code}\n#getting the mean jump height for plotting the linear regression line\nmean_jump_data <- jump_data |> \n  group_by(shoe_color) |> \n  summarize(mean_jump_height = mean(jump_height))\n\n#adding example regression line to previous plot\nggplot(jump_data, aes(x = shoe_color, y = jump_height, color = shoe_color)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) + \n  geom_jitter() +\n  scale_color_manual(values = c(Blue = \"#00BFC4\", Red = \"#F8766D\")) +\n  labs(\n    x = \"Shoe color\",\n    y = \"Vertical jump (inches)\",\n  ) +\n  theme_tufte() +\n  theme(legend.position = \"none\") +\n   geom_line(data = mean_jump_data, #adding example regression line\n            aes(x = shoe_color, y = mean_jump_height, group = 1),\n            linewidth = 1.2,\n            color = \"darkgray\",\n            lineend = \"round\")\n```\n\n::: {.cell-output-display}\n![](ttestsarelinearmodels_files/figure-html/Linear regression and visualization-1.png){width=672}\n:::\n:::\n\n\nNow, interpreting the results of the linear regression, we can see that $\\hat{\\beta}_0$ = 26.7584, in other words, the estimated average jump height for players wearing blue shoes is ≤ 26.7584 inches. Additionally, $\\hat{\\beta}_1x$ = 3.0058, in other words, the average jump height of players wearing red shoes is ≤ 3.0058 inches.\n\n# How this all connects\n\nLet's pull up the results of both the t.test and the linear regression. Can you see what the similarities are? Before moving on, take a closer look at both the results, and then scroll down.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(jump_height ~ shoe_color, data = jump_data, var.equal = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  jump_height by shoe_color\nt = -4.4733, df = 78, p-value = 2.586e-05\nalternative hypothesis: true difference in means between group Blue and group Red is not equal to 0\n95 percent confidence interval:\n -4.343579 -1.668066\nsample estimates:\nmean in group Blue  mean in group Red \n          26.75840           29.76422 \n```\n\n\n:::\n\n```{.r .cell-code}\nlm(formula = jump_height ~ shoe_color, data = jump_data) |> \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = jump_height ~ shoe_color, data = jump_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.795 -1.972 -0.425  1.714  8.489 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    26.7584     0.4751  56.317  < 2e-16 ***\nshoe_colorRed   3.0058     0.6720   4.473 2.59e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.005 on 78 degrees of freedom\nMultiple R-squared:  0.2042,\tAdjusted R-squared:  0.194 \nF-statistic: 20.01 on 1 and 78 DF,  p-value: 2.586e-05\n```\n\n\n:::\n:::\n\n\nWhen you dig into both results, you stumble upon some similarities:\n\n-   **The y-intercept**: Remember $\\hat{\\beta}_0$ from our regression model? It's 26.7584. Take a peek at the mean jump height for the blue shoe group. Surprise, surprise! It's exactly the same number.\n\n-   **The slope coefficient:** Now, let's look at $\\hat{\\beta}_1$, our slope from the regression. It's 3.0058. Here's where it gets interesting. Grab your the means from both groups in t-test results:\n\n    -   Red shoe group mean: 29.76422\n\n    -   Blue shoe group mean: 26.75480\n\n    -   Do a quick subtraction: 29.76422 - 26.75480 = 3.00582 Look familiar? It's our regression slope!\n\n# Why this matters\n\nHopefully this example helped expand your view on statistics a bit. Viewing statistics as a bunch of inflexible rules and tests can obscure the forest for the trees, and a connection like this reveals a hidden harmony in mathematics as a whole. By understanding these connections, you might be able to start crafting solutions to problems, rather than forcing square pegs into round holes.\n",
    "supporting": [
      "ttestsarelinearmodels_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}