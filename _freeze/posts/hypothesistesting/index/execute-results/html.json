{
  "hash": "ec0ea389a9fd4d6c9b2b1f5ecd5a68d3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"So, hypothetically, what would we expect to happen?\"\nauthor: \"Louie Neri\"\ndate: \"07/06/2024\"\nformat:\n  html:\n    toc: true\n    mermaid:\n      theme: default\n    code-fold: true\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ggthemes)\n\nset.seed(123)\n\n#generate data\n\nnull <- rnorm(1000, mean = 0, sd = 1)\nalt <- rnorm(1000, mean = 2, sd = 1)\n\n#create dataframe\ndf <- data.frame(\n  null = null,\n  alt = alt\n)\n\n#create long dataframe\ndf_long <- df |> \n  pivot_longer(cols = everything(),\n               names_to = \"distribution\",\n               values_to = \"value\")\n\n#plot distribution\nggplot(df_long, aes(x = value, fill = distribution)) +\n  geom_density(alpha = 0.5) +\n   geom_vline(xintercept = 0, linetype = \"dashed\", color = \"#2A9D8F\", linewidth = 1) +\n  geom_vline(xintercept = 2, linetype = \"dashed\", color = \"#E76F51\", linewidth = 1) +\n  theme_tufte() +\n  theme(\n    legend.position = \"none\",\n    axis.title = element_blank()\n  ) +\n  annotate(\"text\", x = -0.6, y = 0.2, label = \"Null\\nHypothesis\", \n           color = \"#2A9D8F\", size = 3, fontface = \"bold\") +\n  annotate(\"text\", x = 2.65, y = 0.2, label = \"Alternative\\nHypothesis\", \n           color = \"#E76F51\", size = 3, fontface = \"bold\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n# Null hypothesis significance testing\n\nStarting hypothesis testing is usually when you really start to feel like a scientist. You're usually given this framework, called Null Hypothesis Significance Testing (NHST):\n\n1.  Set up a null hypothesis, which is always that there is \"no difference\" or \"no zero correlation\" or \"zero association.\n2.  Have an alternative hypothesis, which is always contradictory of the null.\n3.  Set 5% as a convention for rejection the null.\n4.  Run the experiment and report p-values as an inequality (e.g., p \\< 0.05, p \\< 0.001)\n5.  Do this for every experiment forever and ever.\n\nNow, this process has been largely critiqued for a couple of reasons. One of them, being the Bayesians who feel that frequentist statistics is the root of all evil. Another reason come from the more conventional/frequentist statisticians who say that it is simply a misrepresentation of how this process was initially developed. I'm a statistical agnostic, and find myself more in the latter camp in this whole debacle.\n\nLet's think about the history of how hypothesis testing began, and why this process outlined above isn't quite what the creators intended.\n\n::: callout-note\nThis blog post was inspired through a combination of this paper, [Mindless Statistics by Gerd Gigerenzer (2004)](https://www.sciencedirect.com/science/article/abs/pii/S1053535704000927), and my wonderful former co-worker, [Greg Lopez of Examine.com](https://examine.com/about/#gregorylopez)\n:::\n\n# Enter, Ronald Fisher\n\nRonald Fisher first came up with this idea of p-values back in the 1920's, and came up with a whole process for testing hypothesis in the 1950s, which is as follows:\n\n1.  Set up a null hypothesis (this does not have to be a nil hypothesis, that there's no difference or no association).\n2.  Run the experiment and report the exact level of significance/p-value (e.g., p = 0.0412, NOT p \\< 0.05)\n3.  If the p-value is below a \"significance level\" one of two things happened\n    -   The null hypothesis is true but you got a rare event\n    -   The null hypothesis or other assumptions were wrong.\n\nFisher's approach to hypothesis testing differs significantly from NHST. In Fisher's view, there isn't a formal \"alternative\" hypothesis, and the concept of a fixed significance level, such as 5%, was more of a recommendation than a strict rule. As my former coworker Greg Lopez of Examine.com insightfully noted, Fisher's methods are quite \"artsy\" and particularly useful for exploratory research where theory isn't well-established.\n\nIn Fisher's paradigm, p-values are interpreted as measures of the strength of evidence against the null hypothesis; the lower the p-value, the more strongly the result contradicts the null. Importantly, the \"null\" hypothesis isn't always a \"nil\" hypothesis (i.e., assuming no effect), but rather a hypothesis to be \"nullified.\" It represents what we would expect to happen under a specific circumstance, and the experiment aims to nullify this expectation. While researchers set a significance level in their minds beforehand, there isn't a hard cutoff like in NHST or what we're about to learn about next.\n\n# Neyman-Pearson, can't we all just get along?\n\nFisher's sworn enemies, Jerzey Neyman and Ergon Pearson, were highly critical of Fisher. They had another view of this whole process, and came up with this method of hypothesis testing:\n\n1.  Set up two hypothesis, your main hypothesis (can be null) and the alternative hypothesis, and decide your $\\alpha$ and $\\beta$ levels before hand, which defines your rejection region\n2.  Run the experiment, and if the data falls into the rejection region, you accept the alternative hypothesis, otherwise, accept the main hypothesis.\n    -   You are only ACTING as if it were true, not truly believing in it.\n\nThis seems to be much more involved and requires more prior information compared to Fisher's approach. In this case, you technically don't need a p-value, but instead, just the test statistic. This approach, can be very useful in fields with strong theory and/or the ability to repeat experiments or procedures over and over (think manufacturing or physics).\n\nIn the Neyman-Pearson approach, there is a hard cutoff determined ahead of time, to create a binary decision, acting as if a hypothesis were true. The hard cutoff requires some thinking on your end, and really depends on your scenario.\n\n# The Frankenstein combination of the two\n\nAfter learning about the Fisher and Neyman-Pearson approaches, you'll see some similarities between the two and NHST.\n\n1.  Set up a null hypothesis, which is always that there is \"no difference\" or \"no zero correlation\" or \"zero association.\n2.  Have an alternative hypothesis, which is always contradictory of the null.\n3.  Set 5% as a convention for rejection the null.\n4.  Run the experiment and report p-values as an inequality (e.g., p \\< 0.05, p \\< 0.001)\n5.  Do this for every experiment forever and ever.\n\nNotice, the first step includes a null hypothesis, but the hypothesis does NOT have to be a nil hypothesis, according to the Fisher method. Additionally, this \"null-hypothesis\" in the first step isn't really anywhere in the Neyman-Pearson method, which focuses on a \"main hypothesis.\"\n\n# So, what am I supposed to do?\n\nUnderstanding the history of how these tests were developed will allow you to be a bit more flexible in interpreting results and/or running experiments. If you're in a field that has relatively weak theory, then perhaps you can interpret the results through the Fisher lens, and be a bit more flexible in how much the data have \"nullified\" the hypothesis. If you're in a field that has a strong theory, or need to make binary (yes/no) decisions, then perhaps we can swim in the Neyman-Pearson waters for a bit, and act as if your hypothesis is true.\n\nBut mixing the two using NHST would not be ideal, as these two methods stem from fundamentally different philosophies, and can lead to misinterpretations of your findings or poor study design. Or worse, you can make a statistician die inside, just a tiny bit.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}