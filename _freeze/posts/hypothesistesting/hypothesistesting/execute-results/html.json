{
  "hash": "3de5789afa5b38c232c6a0e372f23f93",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"So, hypothetically, what would we expect to happen?\"\nauthor: \"Louie Neri\"\ndate: \"07/06/2024\"\ndraft: true\nformat:\n  html:\n    toc: true\n    mermaid:\n      theme: default\n    code-fold: true\n---\n\n\n# Null hypothesis significance testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggthemes)\n\nset.seed(123)\n\n#generate data\n\nnull <- dnorm(1000, mean = 0, sd = 1)\nalt <- dnorm(1000, mean = 2, sd = 1)\n\ndf <- data.frame()\n```\n:::\n\n\nHypothesis testing is usually when you really start to feel like a scientist. You're usually given this framework, called Null Hypothesis Significance Testing (NHST):\n\n1.  Set up a null hypothesis, which is usually that there is \"no difference\" or \"no zero correlation\" or \"zero association.\n2.  Have an alternative hypothesis, which is usually contradictory of the null.\n3.  Set 5% as a convention for rejection the null.\n4.  Run the experiment and report p-values as an inequality (e.g., p \\< 0.05, p \\< 0.001)\n5.  Do this for every experiment forever and ever.\n\nNow, this process has been largely critiqued for a couple of reasons One of them, being the Bayesians who feel that frequentist stats is the root of all evil. Another reason come from the more conventional/frequentist statisticians who say that it is simply a misrepresentation of how this process was initially developed. I'm more generally a statistical agnostic, and find myself more in the latter camp in this whole debacle.\n\nLet's think about the history of how hypothesis testing began, and why this process outlined above isn't quite what the creators intended.\n\n# Ronald Fisher and p-values\n\nRonald Fisher first came up with this idea of p-values back in the 1920's, and came up with a whole process for testing hypothesis in the 1950s, which is as follows:\n\n1.  Set up a null hypothesis (this does not have to be a nil hypothesis, that there's no difference or no association).\n2.  Run the experiment\n3.  Run the experiment and report the exact level of significance/p-value\n4.  If the p-value is below a \"significance level\" one of two things happened\n    -   The null hypothesis is true but you got a rare event\n    -   The null hypothesis or other assumptions were wrong.\n\nNotice, there's no \"alternative\" hypothesis and there isn't really anything about setting 5% as a significance level. Rather, he gave 5% as a \"recommendation\" for setting your significance. He viewed p-values, as measures stronger evidence against the null. As my wonderful former co-worker and stoic, Greg Lopez of Examine.com, taught me, Fisher's methods are very \"artsy\" and useful for exploratory research where theory isn't that strong.\n\nIn the Fisher world, the lower the p-value, the more strongly your result contradicts the null hypothesis. Additionally, the \"null\" hypothesis isn't always a \"nil\" hypothesis, but rather, it is a hypothesis that is to be \"nullified\". Significance is set in your \"head\" ahead of time, but there isn't hard cutoff.\n\n# \"Neyman-Pearson entered the chat\"\n\nFisher's sworn enemies, Jerzey Neyman and Ergon Pearson had another view of this whole process, and came up with this method of hypothesis testing:\n\n1.  Set up two hypothesis, your main hypothesis (can be null) and the alternative hypothesis, and decide your $\\alpha$ and $\\beta$ levels before hand, which defines your rejection region\n2.  Run the experiment, and if the data falls into the rejection region, you accept the alternative hypothesis, otherwise, accept the main hypothesis.\n    -   You are only ACTING as if it were true, not truly believing in it.\n\nThis seems to be much more involved and requires much more prior information compared to Fisher's approach. In this case, you technically don't truly need a p-value, but instead, just need whatever test statistic you get, although you can get one if you'd like. This approach, can be very useful in fields with the ability to repeat experiements or procedures over and over (think manufacturing or physics).\n\nIn the Neyman-Pearson approach, there is a hard cutoff determined ahead of time, to create a binary decision, acting as if a hypothesis were true. The hard cutoff requires some thinking on your end, and really depends on your scenario.\n\n# The frakenstein combination of the two\n\nHypothesis testing is usually when you really start to feel like a scientist. You're usually given this framework, called Null Hypothesis Significance Testing (NHST):\n\n1.  Set up a null hypothesis, that there is \"no difference\" or \"no zero correlation\" or \"zero association.\n2.  Have an alternative hypothesis, which is usually contradictory of the null.\n3.  Set 5% as a convention for rejecting the null.\n4.  Run the experiment and report p-values as an inequality (e.g., p \\< 0.05, p \\< 0.001)\n5.  Do this for every experiment forever and ever.\n\nAfter learning about the Fisher and Neyman-Pearson approaches, you'll see some similarities between the two and NHST. Notice, the first step includes a null hypothesis, but the hypothesis does NOT have to be a nil hypothesis, according to the Fisher method. Additionally, this \"null-hypothesis\" in the first step isn't really anywhere in Neyman and Pearson's methods, which focuses on a \"main hypothesis.\"\n\n# So, what am I supposed to do?\n\nUnderstanding the history of how these tests were developed will allow you to be a bit more flexible in interpreting results or running experiments. If you're in a field that has relatively weak theory, then perhaps you can interpret the results through the Fisherian lens, and be a bit more flexible in how much the data have \"nullified\" the hypothesis. If you're in a field that has a strong theory, or need to make binary (yes/no) decisions, then perhaps we can swim in the Neyman-Pearson waters for a bit, and act as if your hypothesis is true.\n\nBut mixing the two using NHST would not be ideal, as these two methods had fundamentally different philosophies, and can lead to some poor misinterpretations of research and not carefully designed studies.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}