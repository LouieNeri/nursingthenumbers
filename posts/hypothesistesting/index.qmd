---
title: "So, hypothetically, what would we expect to happen?"
author: "Louie Neri"
date: "07/06/2024"
format:
  html:
    toc: true
    mermaid:
      theme: default
    code-fold: true
---

```{r}
#| message: FALSE

library(tidyverse)
library(ggthemes)

set.seed(123)

#generate data

null <- rnorm(1000, mean = 0, sd = 1)
alt <- rnorm(1000, mean = 2, sd = 1)

#create dataframe
df <- data.frame(
  null = null,
  alt = alt
)

#create long dataframe
df_long <- df |> 
  pivot_longer(cols = everything(),
               names_to = "distribution",
               values_to = "value")

#plot distribution
ggplot(df_long, aes(x = value, fill = distribution)) +
  geom_density(alpha = 0.5) +
   geom_vline(xintercept = 0, linetype = "dashed", color = "#2A9D8F", linewidth = 1) +
  geom_vline(xintercept = 2, linetype = "dashed", color = "#E76F51", linewidth = 1) +
  theme_tufte() +
  theme(
    legend.position = "none",
    axis.title = element_blank()
  ) +
  annotate("text", x = -0.6, y = 0.2, label = "Null\nHypothesis", 
           color = "#2A9D8F", size = 3, fontface = "bold") +
  annotate("text", x = 2.65, y = 0.2, label = "Alternative\nHypothesis", 
           color = "#E76F51", size = 3, fontface = "bold") 
```

# Null hypothesis significance testing

When you start to conduct hypothesis testing, you really start to feel like a scientist. You're usually given this framework for testing hypotheses, called Null Hypothesis Significance Testing (NHST). It goes something like this:

1.  Set up a null hypothesis, which is always that there is "no difference" or "no correlation" or "no association.
2.  Have an alternative hypothesis, which is always contradictory of the null hypothesis.
3.  Always set 5% as the significance level for rejecting the null hypothesis.
4.  Run the experiment and report p-values as an inequality if it's significant (e.g., p \< 0.05).
5.  Do this for every experiment forever and ever.

Now, this process has been largely critiqued. One of coming from the Bayesian statisticians who feel that frequentist statistics is the root of all evil. Another critique come from the more conventional/frequentist statisticians who say that it is simply a misrepresentation of how this process was initially developed. I'm a statistical agnostic, and find myself more in the latter camp in this whole debacle.

To understand hypothesis testing, I think it's best to briefly learn about its history.

::: callout-note
This blog post was inspired through a combination of this paper, [Mindless Statistics by Gerd Gigerenzer (2004)](https://www.sciencedirect.com/science/article/abs/pii/S1053535704000927), and my wonderful co-worker/stoic/stats-wizard, [Greg Lopez of Examine.com](https://examine.com/about/#gregorylopez)
:::

# Enter, Ronald Fisher

The statistician Ronald Fisher first came up with this idea of p-values back in the 1920's, and came up with a whole process for testing hypotheses in the 1950s, which is as follows:

1.  Set up a null hypothesis (this does not have to be a nil hypothesis, that there's no difference or no association).
2.  Determine your level of significance (it does *not* have to be 5%)
3.  Run the experiment and report the exact level of significance/p-value (e.g., p = 0.0412, NOT p \< 0.05)
4.  If reaches the significance level you determined, you reject the null hypothesis. If it doesn't, you fail to reject the null hypothesis.

This looks a bit different than what we first outlined from NHST. In Fisher's world, there is no "alternative" hypothesis and the concept of a fixed significance level, such as 5%, was more of a recommendation than a strict rule. As my former coworker Greg Lopez of Examine.com insight-fully noted, Fisher's methods are quite "artsy" and particularly useful for exploratory research where theory isn't well-established.

In Fisher's paradigm, p-values are interpreted as measures of the strength of evidence against the null hypothesis; the lower the p-value, the more strongly the result contradicts the null. Importantly, the "null" hypothesis isn't always a "nil" hypothesis (i.e., assuming no effect), but rather a hypothesis to be "nullified." It represents what we would expect to happen under a specific circumstance, and the experiment aims to nullify this expectation. While researchers set a significance level beforehand, there isn't a hard cutoff like in NHST. However, some other folks disagreed.

# Neyman-Pearson, can't we all just get along?

Fisher's sworn enemies, Jerzey Neyman and Ergon Pearson, had another view of this whole process. They came up with this method of hypothesis testing:

1.  Set up two hypotheses, your main hypothesis (usually a nil hypothesis) and the alternative hypothesis (usually contradictory of the null).
2.  Decide your $\alpha$ (false positive) and $\beta$ (false negative) levels before hand, which defines your rejection region.
3.  Run the experiment, and if the data falls into the rejection region, you accept the alternative hypothesis, otherwise, accept the main hypothesis.
    -   You are only ACTING as if the hypothesis were true, not truly believing in it.

This seems to be much more involved and requires more prior information compared to Fisher's approach. In this case, you technically don't need a p-value, but instead, just the test statistic. This approach, can be very useful in fields with strong theory and/or the ability to repeat experiments or procedures over and over (think manufacturing or physics).

In the Neyman-Pearson approach, there is a hard cutoff determined ahead of time, to create a binary decision, allowing you to act as if a hypothesis were true. The hard cutoff requires some thinking on your end and really depends on how rigorous you want to be.

# The Frankenstein combination of the two

After learning about the Fisher and Neyman-Pearson approaches, you'll see that NHST is a weird combination of the two. Let's look at NHST again:

1.  Set up a null hypothesis, which is always that there is "no difference" or "no correlation" or "no association."
2.  Have an alternative hypothesis, which is always contradictory of the null.
3.  Set 5% as a convention for rejection the null.
4.  Run the experiment and report p-values as an inequality (e.g., p \< 0.05, p \< 0.001)
5.  Do this for every experiment forever and ever.

Notice, the first step includes a null hypothesis, but the hypothesis does NOT have to be a nil hypothesis, but in NHST, it always has to be a nil hypothesis. Additionally, this "null-hypothesis" in the first step isn't really anywhere in the Neyman-Pearson method, which focuses on a "main hypothesis." Do you see any other parts where NHST overlaps with Fisher and Neyman-Pearson's methods?

# So, what am I supposed to do?

Understanding the history of how these tests were developed will allow you to be a bit more flexible in interpreting results and/or running experiments. If you're in a field that has relatively weak theory, then perhaps you can interpret results through the Fisher lens, and be a bit more flexible in how much the data have "nullified" the hypothesis. If you're in a field that has a strong theory, or need to make binary (yes/no) decisions, then perhaps we can swim in the Neyman-Pearson waters for a bit, and act as if your hypothesis is true.

But mixing the two using NHST would not be ideal, as these two methods stem from fundamentally different philosophies, and can lead to misinterpretations of your findings or poor study design. Or worse, you can make a statistician die inside, just a tiny bit.
