[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Louie Neri",
    "section": "",
    "text": "Hello there! I’m a nurse researcher with a passion for biostatistics, epidemiology, health economics, nutrition, exercise science, and mental health.\nI earned my BSN with Research Distinction from UT Health San Antonio, where I researched the impact of nutrition and lifestyle on stroke, diabetes, and cognitive decline.\nCurrently, I’m the Senior Coordinator for Evidence Based Surgery at The Society of Thoracic Surgeons, where I’m lucky enough to work with some outstanding surgeons to develop clinical practice guidelines. I’m also an incoming MSPH student in Outcomes Research at the University of Alabama at Birmingham. Previously, I was a Researcher and Team Lead at Examine.com, the largest online nutrition database."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Louie Neri",
    "section": "Education",
    "text": "Education\nUniversity of Alabama at Birmingham | Birmingham, AL (Remote)\nMasters of Science in Public Health (MSPH) | August 2024 – December 2026\nUniversity of Texas Health Science Center at San Antonio | San Antonio, TX\nBachelors of Science in Nursing with a Distinction in Research (BSN-RD) | August 2020 – May 2022"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Louie Neri",
    "section": "Experience",
    "text": "Experience\nSenior Coordinator, Evidence Based Surgery | The Society of Thoracic Surgeons | September 2023 – Current\nResearch Team Lead | Examine.com | December 2022 – September 2023\nResearcher | Examine.com | May 2021 – December 2022\nUndergraduate Researcher | Audie Murphy VA Hospital/UT Health San Antonio | October 2022 – May 2022"
  },
  {
    "objectID": "about.html#some-kind-words-from-a-close-mentor",
    "href": "about.html#some-kind-words-from-a-close-mentor",
    "title": "Louie Neri",
    "section": "Some kind words from a close mentor:",
    "text": "Some kind words from a close mentor:\n\n“It is rare to find a human as inquisitive as Louie. He has the soul of a scientist, the heart of a physician, and the cleverness of a crow. His dedication to self-improvement through understanding the scientific principles of biochemistry and research methods is unparalleled. It was a true joy to have Louie in my courses as we constantly encourage mutual curiosity by debating the latest science. I am grateful to count him as a student and friend, and I am honored for him to be a professional colleague in the scientific community.”\n\nDeana Apple, PhD"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The meanderings of a nurse trying to make sense of science",
    "section": "",
    "text": "So, hypothetically, what would we expect to happen?\n\n\n\n\n\n\nLouie Neri\n\n\nJul 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nI am 95% confident that I don’t know what a confidence interval is\n\n\n\n\n\n\nLouie Neri\n\n\nJun 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nT-tests are just linear models\n\n\n\n\n\n\nLouie Neri\n\n\nJun 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe beginning…\n\n\n\n\n\n\nLouie Neri\n\n\nJun 19, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "The beginning…",
    "section": "",
    "text": "Edward Munch, The Sick Child, 1907 © CC-BY-NC-ND 3.0"
  },
  {
    "objectID": "posts/welcome/index.html#how-this-madness-began",
    "href": "posts/welcome/index.html#how-this-madness-began",
    "title": "The beginning…",
    "section": "How this madness began",
    "text": "How this madness began\nGreetings! Welcome to the inaugural post of my blog “Nursing the numbers.”\nAs a quick introduction, I’m Louie Neri, a nurse who never ended up nursing, somwhow ended up in the nutrition science world, and now find myself in the clinical practice guideline universe. I wanted to give a brief introduction to myself, how I got here, and the goal of this blog.\n\nHealth issues and functional medicine\nI first got interested in science because of a myriad of health issues that had me bed-ridden in high school. Although I will spare the details (perhaps I can explain this for another time), I ended up dislocating my shoulder playing basketball, resulting in a paradoxical bilateral frozen shoulder, could not stand up straight without feeling like I was going to pass out, covered in severe (and I mean severe eczema), severe gastritis, and irritable bowel syndrome (IBS), concurrently. All at the ripe age of 17.\nAll I had left was free time, and my laptop. Scouring the internet for answers, I ended up in engrossed in the functional medicine world, trying to teach myself as much about the human body as possible. I found a local functional medicine doctor in my town, who miraculously healed me with a combination of dietary supplements and meditation. After trying a few different career paths, this experience led me to go to nursing school, with the hopes of becoming some sort of holistic-yet-scientific nurse practitioner.\n\n\nReturning to mainstream medicine and\nNow, this isn’t some sort of miracle healing story. My ailments began to return while getting my pre-requisite classess for nursing school, which involved a bunch of basic science classes.\nTaking these classes made me realize how much I did not know, and was better able to evaluate the claims commonly made in health research. I ended up being disengaged with functional medicine as a whole, realizing how poor the research truly was.\nWhen I went to nursing school, I got lucky to go to a huge academic research institution, where we had a “research distinction” program. By sheer luck, I got matched with a nutrition health scientist as my advisor, where I was able to see how science really worked.\nI now find myself more aligned with more “mainstream” medicine (whatever that means). I’m now interested more in the big picture of how our healthcare system works, as opposed to the nitty gritty details of treating patients (although I do miss that work).\nAfter graduating with my BSN and doing some nutrition research, I ended up working for the amazing company Examine.com, and now I develop clinical practice guidelines for a medical association."
  },
  {
    "objectID": "posts/welcome/index.html#the-purpose-of-this-blog",
    "href": "posts/welcome/index.html#the-purpose-of-this-blog",
    "title": "The beginning…",
    "section": "The purpose of this blog",
    "text": "The purpose of this blog\nThe main purpose of this blog is to act as a diary of sorts while I think about or teach myself things related to biostatistics, epidemiology, health economics, and science as a whole. My hope is that, in a few years or so, I will look back at what I’ve written and be proud of how much I’ve learned, and realize how much I didn’t know.\nPlease don’t take anything I say here as absolute truth, and please let me know if I’m horribly wrong on anything. I probably will be wrong, and I’m learning to be okay with that."
  },
  {
    "objectID": "posts/welcome/index.html#how-this-all-began",
    "href": "posts/welcome/index.html#how-this-all-began",
    "title": "The beginning…",
    "section": "How this all began",
    "text": "How this all began\nGreetings! Welcome to the inaugural post of my blog “Nursing the Numbers.”\nAs a quick introduction, I’m Louie Neri, a nurse who never ended up nursing, who somehow ended up in the nutrition science world, and now finds himself in the clinical practice guideline extended universe. I wanted to give a brief introduction to myself, how I got here, and the goal of this blog.\n\nHealth issues and functional medicine\nI first got interested in science because of a myriad of health issues that had me bed-ridden in high school. All within the span of a few months, I ended up dislocating my shoulder playing basketball, resulting in a paradoxical bilateral frozen shoulder, could not stand up straight without feeling like I was going to pass out, covered in severe eczema, could barely handle any food due to severe gastritis, and spent way too much time in the bathroom with irritable bowel syndrome (IBS).\nAll I had left was free time, and my laptop. Scouring the internet for answers, I ended up in engrossed in the functional medicine world, trying to teach myself as much about the human body as possible. I found a local functional medicine doctor in my town, who miraculously healed me with a concoction dietary supplements and meditation. After trying a few different career paths, this experience led me to go to nursing school, with the hopes of becoming some sort of holistic-yet-scientific nurse practitioner.\n\n\nReturning to mainstream medicine\nNow, this isn’t some sort of miracle healing story. My ailments began to return while getting my pre-requisite courses for nursing school, which involved a bunch of basic science classes.\nTaking these classes made me realize how much I did not know. I was better able to evaluate the claims commonly made in health research and ended up disassociating myself from functional medicine in general.\nWhen I went to nursing school, I got lucky to attend a huge academic research institution, where we had a “research distinction” program. By sheer luck, I got matched with a nutrition health scientist as my advisor, where I was able to see how science really worked.\n\n\nWhere I’m at now\nI now find myself more aligned with “mainstream” medicine (whatever that means). I’m now more interested in the big picture of how our healthcare system works, as opposed to the nitty gritty clinical details of treating patients (although I do miss that work).\nAfter graduating with my BSN and doing some nutrition research, I ended up working for the amazing company Examine.com for a couple years, and now I develop clinical practice guidelines for a medical association."
  },
  {
    "objectID": "posts/ttestsarejustlinearmodels/ttestsarelinearmodels.html",
    "href": "posts/ttestsarejustlinearmodels/ttestsarelinearmodels.html",
    "title": "T-tests are just linear models",
    "section": "",
    "text": "The madness of statistical tests\nIn your introductory statistics class, you’re introduced to a myriad of statistical tests that are confusing and have weird names of scientists you’ve never heard of. You do some Googling or look in the appendix of your textbook, and you find a neat graph that looks something like this:\n\n\n\n\n\nflowchart TD\n  subgraph Parametric_assumptions[Parametric assumptions]\n        direction TB\n        Z1[1. Independent samples]\n        Z2[2. Data normally distributed]\n        Z3[3. Equal variances]\n    end\n\n    A[Type of data?] --&gt;|Continuous| B[Type of question?]\n    A --&gt;|Discrete, categorical| C[Any counts &lt; 5?]\n    \n    B --&gt;|Relationships| D[Do you have dependent & independent variables?]\n    B --&gt;|Differences| E[Differences between what?]\n    \n    C --&gt;|No| F[Chi-square tests, one and two sample]\n    C --&gt;|Yes| G[Fisher's exact test]\n    \n    D --&gt;|Yes| H[Regression analysis]\n    D --&gt;|No| I[Correlation analysis]\n    \n    I --&gt;|Parametric| J[Pearson's r]\n    I --&gt;|Nonparametric| K[Spearman's rank correlation]\n    \n    E --&gt;|Means| L[One-sample t-test]\n    E --&gt;|Variances| M[Fmax test or Bartlett's test]\n    E --&gt;|Multiple means Single variable| N[How many groups?]\n    \n    N --&gt;|Two| O[Parametric assumptions satisfied?]\n    N --&gt;|More than two| P[Parametric assumptions satisfied?]\n    \n    O --&gt;|Yes| Q[Student's t-test]\n    O --&gt;|No| R[Transform data?]\n    R --&gt;|OK| Q\n    R --&gt;|No| S[Mann-Whitney U or Wilcoxon test]\n    \n    P --&gt;|Yes| T[One-way ANOVA]\n    P --&gt;|No| U[Transform data?]\n    U --&gt;|No| V[Kruskall-Wallis test]\n    U --&gt;|OK| T\n    \n    T & V --&gt; W[If significant, do post hoc test: Bonferroni's, Dunn's, Tukey's, etc.]\n    \n    \n\n\n\n\n\n\nGreat! Now, you know exactly which test you need to do and when to do it. However, this is probably not the best statistical practice. For some if not most people, this will probably get the job done. But, you may find yourself in scenarios with messy data that do not fit neatly into the flow chart, leading you to inadvertently violate some crucial assumption of the statistical test, and your wonderfully designed study goes in the trash.\nLet me introduce you to the linear model! Believe it or not, most statistical tests are really just variations of the linear model. To understand this, let’s simulate some data and see how a simple two sample t-test is really a linear model!\n\n\nTesting the world of shoe color and dunking\nHere’s the scenario: we live in a hypothetical world where shoe color affects the vertical of basketball players. In this case, red shoes allow players to jump higher when compared to blue shoes. Since I cannot dunk a basketball (I’m 5’7”) and would love for this to be true, I’m going to put this to the test.\nI’ve recruited 80 players to study this shoe, allocating 40 players to wear the blue shoes and 40 to wear the red shoes. Each player records a vertical jump test, and their best height is recorded. Let’s simulate this data and create a box plot to see how it looks.\n\n\nCode\n#setting the parameters\nset.seed(1234)\n\nn_players &lt;- 40\nblue_mean &lt;- 28\nred_mean &lt;- 30\nsd &lt;- 3\n\n#simulate jump heights in inches\nblue_jump_height &lt;- rnorm(n_players, blue_mean, sd)\nred_jump_height &lt;- rnorm(n_players, red_mean, sd)\n\n#create a dataframe\njump_data &lt;- data.frame(\n  player_id = 1:(2 * n_players),\n  shoe_color = rep(c(\"Blue\", \"Red\"), each = n_players),\n  jump_height = c(blue_jump_height, red_jump_height)\n)\n\n#plot the data\nggplot(jump_data, aes(x = shoe_color, y = jump_height, color = shoe_color)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) + \n  geom_jitter() +\n  scale_color_manual(values = c(Blue = \"#00BFC4\", Red = \"#F8766D\")) +\n  labs(\n    x = \"Shoe color\",\n    y = \"Vertical jump (inches)\",\n  ) +\n  theme_tufte() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nIntuitively, we can tell that the blue shoes indeed increase vertical jump. However, let’s verify this with the good ole two sample t-test.\n\n\nCode\nt.test(jump_height ~ shoe_color, data = jump_data, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  jump_height by shoe_color\nt = -4.4733, df = 78, p-value = 2.586e-05\nalternative hypothesis: true difference in means between group Blue and group Red is not equal to 0\n95 percent confidence interval:\n -4.343579 -1.668066\nsample estimates:\nmean in group Blue  mean in group Red \n          26.75840           29.76422 \n\n\nNow, another way to test this is to view the problem in the context of a simple linear regression, with a binary predictor. In this case, the binary predictor would be blue shoes or red shoes. First, let’s review the formula for a simple linear regression.\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x\n\\]\nIf you’re a bit rusty:\n\n\\(\\hat{y}\\) represents is the estimated dependent variable, in this case, vertical jump height dependent on \\(x\\) (in this case, shoe color).\n\\(\\hat{\\beta}_0\\) represents the \\(\\hat{y}\\) intercept when x = 0, but in this case, it would be for those who are in the blue shoe group.\n\\(\\hat{\\beta}_1x\\) represents the estimated slope coefficient, which tell us that how much \\(y\\) (jump height) changes, on average, for a one-unit increase in \\(x\\).\n\nWhat you’ll see in a moment, is that fitting a regression line using this binary predictor of shoe color, is the same as doing a two sample t-test.\nLet’s see what happens when we compute the linear regression.\n\n\nCode\n#the regression model\nlm(formula = jump_height ~ shoe_color, data = jump_data) |&gt; \n  summary()\n\n\n\nCall:\nlm(formula = jump_height ~ shoe_color, data = jump_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.795 -1.972 -0.425  1.714  8.489 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    26.7584     0.4751  56.317  &lt; 2e-16 ***\nshoe_colorRed   3.0058     0.6720   4.473 2.59e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.005 on 78 degrees of freedom\nMultiple R-squared:  0.2042,    Adjusted R-squared:  0.194 \nF-statistic: 20.01 on 1 and 78 DF,  p-value: 2.586e-05\n\n\nCode\n#getting the mean jump height for plotting the linear regression line\nmean_jump_data &lt;- jump_data |&gt; \n  group_by(shoe_color) |&gt; \n  summarize(mean_jump_height = mean(jump_height))\n\n#adding example regression line to previous plot\nggplot(jump_data, aes(x = shoe_color, y = jump_height, color = shoe_color)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) + \n  geom_jitter() +\n  scale_color_manual(values = c(Blue = \"#00BFC4\", Red = \"#F8766D\")) +\n  labs(\n    x = \"Shoe color\",\n    y = \"Vertical jump (inches)\",\n  ) +\n  theme_tufte() +\n  theme(legend.position = \"none\") +\n   geom_line(data = mean_jump_data, #adding example regression line\n            aes(x = shoe_color, y = mean_jump_height, group = 1),\n            linewidth = 1.2,\n            color = \"darkgray\",\n            lineend = \"round\")\n\n\n\n\n\n\n\n\n\nNow, interpreting the results of the linear regression, we can see that \\(\\hat{\\beta}_0\\) = 26.7584, in other words, the estimated average jump height for players wearing blue shoes is ≤ 26.7584 inches. Additionally, \\(\\hat{\\beta}_1x\\) = 3.0058, in other words, the average jump height of players wearing red shoes is ≤ 3.0058 inches.\n\n\nHow this all connects\nLet’s pull up the results of both the t.test and the linear regression. Can you see what the similarities are? Before moving on, take a closer look at both the results, and then scroll down.\n\n\nCode\nt.test(jump_height ~ shoe_color, data = jump_data, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  jump_height by shoe_color\nt = -4.4733, df = 78, p-value = 2.586e-05\nalternative hypothesis: true difference in means between group Blue and group Red is not equal to 0\n95 percent confidence interval:\n -4.343579 -1.668066\nsample estimates:\nmean in group Blue  mean in group Red \n          26.75840           29.76422 \n\n\nCode\nlm(formula = jump_height ~ shoe_color, data = jump_data) |&gt; \n  summary()\n\n\n\nCall:\nlm(formula = jump_height ~ shoe_color, data = jump_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.795 -1.972 -0.425  1.714  8.489 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    26.7584     0.4751  56.317  &lt; 2e-16 ***\nshoe_colorRed   3.0058     0.6720   4.473 2.59e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.005 on 78 degrees of freedom\nMultiple R-squared:  0.2042,    Adjusted R-squared:  0.194 \nF-statistic: 20.01 on 1 and 78 DF,  p-value: 2.586e-05\n\n\nWhen you dig into both results, you stumble upon some similarities:\n\nThe y-intercept: Remember \\(\\hat{\\beta}_0\\) from our regression model? It’s 26.7584. Take a peek at the mean jump height for the blue shoe group. Surprise, surprise! It’s exactly the same number.\nThe slope coefficient: Now, let’s look at \\(\\hat{\\beta}_1\\), our slope from the regression. It’s 3.0058. Here’s where it gets interesting. Grab your the means from both groups in t-test results:\n\nRed shoe group mean: 29.76422\nBlue shoe group mean: 26.75480\nDo a quick subtraction: 29.76422 - 26.75480 = 3.00582 Look familiar? It’s our regression slope!\n\n\n\n\nWhy this matters\nHopefully this example helped expand your view on statistics a bit. Viewing statistics as a bunch of inflexible rules and tests can obscure the forest for the trees, and a connection like this reveals a hidden harmony in mathematics as a whole. By understanding these connections, you might be able to start crafting solutions to problems, rather than forcing square pegs into round holes."
  },
  {
    "objectID": "posts/confidenceintervals/confidenceintervals.html",
    "href": "posts/confidenceintervals/confidenceintervals.html",
    "title": "I am 95% confident that I don’t know what a confidence interval is",
    "section": "",
    "text": "Confidence intervals (CI) have been the bane of my scientific existence. They are very unintuitive, and even statisticians debate on how to best describe a confidence interval.\nLet’s dive into the depths of my mind as I try to figure out what in the world a confidence interval is.\n\nConfident about… what?\nThe more specific definition of a 95% confidence interval (using 95% for the sake of convention) is as follows:\n\nIf I were to repeat this sampling process over and over, 95% of the confidence intervals would contain the true parameter value.\n\nNow, here’s where the confusion lies. Intuitively, this would lead you to think that it is the probability that the true value is in that interval. NOPE!\nLet’s look at it another way. Imagine we’re trying to determine the mean height of all NBA players (yes, another basketball reference). Here’s how it might play out:\n\nYou get a random sample of players, calculate the mean height, and determine a 95% confidence interval.\nI decide to do the same thing. I get my own random sample, calculate the mean, and find my own 95% confidence interval.\nThen our friend Alexis joins in. She gets her random sample and calculates her mean and 95% confidence interval.\n\nIf we (and others) kept doing this over and over, about 95% of all these confidence intervals would contain the true mean height of NBA players. However, for any single interval – yours, mine, or Alexis’s – we can’t know for certain whether it’s one of the 95% that contains the true mean or one of the 5% that doesn’t.\nI don’t think it would be very useful to demonstrate this idea in much detail, since Kristoffer Magmnusson’s visualization is more than sufficient and I would recommend playing around with it!\n\n\n\nCourtsey of Kristoffer Magmusson\n\n\nI think it’s more useful to think about how to interpret single confidence intervals. Since, a majority of the time, we’re reading studies that just spit out one confidence interval.\n\n\nInterpreting single confidence intervals\nWhen looking at a confidence interval, it’s best to think of it as a measure of how precise the estimate is, but this does NOT tell you how accurate the estimate is.\n\n\nMy new teaching method\nLet’s simulate a meta-analysis for the sake of example.\nIn this example, we are looking at the effect of my new teaching method called the Learning Optimization and Understanding through Interactive Engagement (LOUIE) Method, compared to traditional teaching methods, on math scores of high school students. Each study represents a different school studying the LOUIE method and comparing it to traditional teaching, and getting the mean test scores for each group.\n\n\nCode\n#load packages\nlibrary(\"metafor\")\n\n#set seed for reproducibility\nset.seed(1234)\n\n# Number of studies\nn &lt;- 5\n\n# Simulate data\nsim_data &lt;- data.frame(\n  study = paste(\"Study\", 1:n),\n  author = c(\"Smith et al.\", \"Johnson et al.\", \"Williams et al.\", \"Brown et al.\", \"Davis et al.\"),\n  year = c(2018, 2019, 2020, 2021, 2022),\n  n1i = c(100, 150, 200, 120, 250),  # sample size group 1\n  n2i = c(110, 140, 190, 130, 250),  # sample size group 2\n  m1i = c(9.2, 9.5, 9.8, 10.2, 8.4),  # mean of group 1\n  m2i = c(9.0, 9.8, 10.3, 10.9, 8.5),  # mean of group 2\n  sd1i = c(2.1, 1.9, 2.3, 2.0, 2.2),  # standard deviation of group 1\n  sd2i = c(2.2, 2.0, 2.1, 2.3, 2.1)   # standard deviation of group 2\n)\n\n#calcuate SMD\ndat &lt;- escalc(measure = \"SMD\",\n                       m1i = m1i,\n                       m2i = m2i,\n                       sd1i = sd1i,\n                       sd2i = sd2i,\n                       n1i = n1i,\n                       n2i = n2i,\n                       data = sim_data)\n\n#view\ndat\n\n\n\n    study          author year n1i n2i  m1i  m2i sd1i sd2i      yi     vi \n1 Study 1    Smith et al. 2018 100 110  9.2  9.0  2.1  2.2  0.0926 0.0191 \n2 Study 2  Johnson et al. 2019 150 140  9.5  9.8  1.9  2.0 -0.1535 0.0139 \n3 Study 3 Williams et al. 2020 200 190  9.8 10.3  2.3  2.1 -0.2263 0.0103 \n4 Study 4    Brown et al. 2021 120 130 10.2 10.9  2.0  2.3 -0.3229 0.0162 \n5 Study 5    Davis et al. 2022 250 250  8.4  8.5  2.2  2.1 -0.0464 0.0080 \n\n\nCode\n#random effects model\nmodel_result &lt;- rma(yi, vi, data = dat)\n\n#create forest plot\nforest(model_result, header = TRUE, xlab = \"LOUIE vs Traditional\")\n\n\n\n\n\n\n\n\n\nIn the meta-analysis above, we’re looking at the standardized mean differences (SMDs) of 5 studies, and their 95% confidence intervals. As we can see, maybe there’s something to the LOUIE method? All this talk about my teaching method is really a distraction, let’s look at the confidence intervals.\nFor the sake of brevity, I will not go into the math behind all of this. But in general, as the sample size increases, the confidence interval will get narrower, and thus, the estimate becomes a bit more precise.\nLet’s play around and increase the sample size of both groups in study 5 from 250 to 500. Take a look at what happens to the confidence interval of that study.\n\n\nCode\n#set seed for reproducibility\nset.seed(1234)\n\n# Number of studies\nn &lt;- 5\n\n# Simulate data\nsim_data &lt;- data.frame(\n  study = paste(\"Study\", 1:n),\n  author = c(\"Smith et al.\", \"Johnson et al.\", \"Williams et al.\", \"Brown et al.\", \"Davis et al.\"),\n  year = c(2018, 2019, 2020, 2021, 2022),\n  n1i = c(100, 150, 200, 120, 500),  # sample size group 1\n  n2i = c(110, 140, 190, 130, 500),  # sample size group 2\n  m1i = c(9.2, 9.5, 9.8, 10.2, 8.4),  # mean of group 1\n  m2i = c(9.0, 9.8, 10.3, 10.9, 8.5),  # mean of group 2\n  sd1i = c(2.1, 1.9, 2.3, 2.0, 2.2),  # standard deviation of group 1\n  sd2i = c(2.2, 2.0, 2.1, 2.3, 2.1)   # standard deviation of group 2\n)\n\n#calcuate SMD\ndat &lt;- escalc(measure = \"SMD\",\n                       m1i = m1i,\n                       m2i = m2i,\n                       sd1i = sd1i,\n                       sd2i = sd2i,\n                       n1i = n1i,\n                       n2i = n2i,\n                       data = sim_data)\n\n#view\ndat\n\n\n\n    study          author year n1i n2i  m1i  m2i sd1i sd2i      yi     vi \n1 Study 1    Smith et al. 2018 100 110  9.2  9.0  2.1  2.2  0.0926 0.0191 \n2 Study 2  Johnson et al. 2019 150 140  9.5  9.8  1.9  2.0 -0.1535 0.0139 \n3 Study 3 Williams et al. 2020 200 190  9.8 10.3  2.3  2.1 -0.2263 0.0103 \n4 Study 4    Brown et al. 2021 120 130 10.2 10.9  2.0  2.3 -0.3229 0.0162 \n5 Study 5    Davis et al. 2022 500 500  8.4  8.5  2.2  2.1 -0.0465 0.0040 \n\n\nCode\n#random effects model\nmodel_result &lt;- rma(yi, vi, data = dat)\n\n#create forest plot\nforest(model_result, header = TRUE, xlab = \"LOUIE vs Traditional\")\n\n\n\n\n\n\n\n\n\nNotice, it’s a bit more narrow. This is because, we’ve increased the sample size, thus increasing the precision of the estimate.\nTo get back to what I said earlier, this does NOT tell us how accurate this estimate is. Since we have no idea whether the studies have other sources of bias, like sampling bias, allocation bias, or measurement bias, that may affect the estimate. Increasing the sample size has increased our “confidence” about the true estimate, and that in the long run, if we do this over and over, 95% of the time, it will contain the true estimate.\n\n\nDo you feel confident?\nOverall, confidence intervals don’t make sense. But, they are useful to give us an idea of how precise an estimate is and their behavior in the long-run across identical studies. I think the confusion stems from the frequentist foundations of confidence intervals, and that our natural intuition does not think in this hypothetical world of constantly repeating experiments.\nIf you really want to get the probability that the true parameter is in the interval, then look into the Bayesian credible interval. But I can save that for another time."
  },
  {
    "objectID": "posts/hypothesistesting/hypothesistesting.html",
    "href": "posts/hypothesistesting/hypothesistesting.html",
    "title": "So, hypothetically, what would we expect to happen?",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggthemes)\n\nset.seed(123)\n\n#generate data\n\nnull &lt;- rnorm(1000, mean = 0, sd = 1)\nalt &lt;- rnorm(1000, mean = 2, sd = 1)\n\ndf &lt;- data.frame(\n  null = null,\n  alt = alt\n)\n\ndf_long &lt;- df |&gt; \n  pivot_longer(cols = everything(),\n               names_to = \"distribution\",\n               values_to = \"value\")\n\nggplot(df_long, aes(x = value, fill = distribution)) +\n  geom_density(alpha = 0.5) +\n   geom_vline(xintercept = 0, linetype = \"dashed\", color = \"#2A9D8F\", linewidth = 1) +\n  geom_vline(xintercept = 2, linetype = \"dashed\", color = \"#E76F51\", linewidth = 1) +\n  theme_tufte() +\n  theme(\n    legend.position = \"none\",\n    axis.title = element_blank()\n  ) +\n  annotate(\"text\", x = -0.6, y = 0.2, label = \"Null\\nHypothesis\", \n           color = \"#2A9D8F\", size = 3, fontface = \"bold\") +\n  annotate(\"text\", x = 2.65, y = 0.2, label = \"Alternative\\nHypothesis\", \n           color = \"#E76F51\", size = 3, fontface = \"bold\") \n\n\n\n\n\n\n\n\n\n\nNull hypothesis significance testing\nHypothesis testing is usually when you really start to feel like a scientist. You’re usually given this framework, called Null Hypothesis Significance Testing (NHST):\n\nSet up a null hypothesis, which is always that there is “no difference” or “no zero correlation” or “zero association.\nHave an alternative hypothesis, which is always contradictory of the null.\nSet 5% as a convention for rejection the null.\nRun the experiment and report p-values as an inequality (e.g., p &lt; 0.05, p &lt; 0.001)\nDo this for every experiment forever and ever.\n\nNow, this process has been largely critiqued for a couple of reasons. One of them, being the Bayesians who feel that frequentist statistics is the root of all evil. Another reason come from the more conventional/frequentist statisticians who say that it is simply a misrepresentation of how this process was initially developed. I’m a statistical agnostic, and find myself more in the latter camp in this whole debacle.\nLet’s think about the history of how hypothesis testing began, and why this process outlined above isn’t quite what the creators intended.\n\n\n\n\n\n\nNote\n\n\n\nThis blog post was inspired through a combination of this paper, Mindless Statistics by Gerd Gigerenzer (2004), and my wonderful former co-worker, Greg Lopez of Examine.com\n\n\n\n\nEnter, Ronald Fisher\nRonald Fisher first came up with this idea of p-values back in the 1920’s, and came up with a whole process for testing hypothesis in the 1950s, which is as follows:\n\nSet up a null hypothesis (this does not have to be a nil hypothesis, that there’s no difference or no association).\nRun the experiment\nRun the experiment and report the exact level of significance/p-value\nIf the p-value is below a “significance level” one of two things happened\n\nThe null hypothesis is true but you got a rare event\nThe null hypothesis or other assumptions were wrong.\n\n\nFisher’s approach to hypothesis testing differs significantly from NHST. In Fisher’s view, there isn’t a formal “alternative” hypothesis, and the concept of a fixed significance level, such as 5%, was more of a recommendation than a strict rule. As my former coworker Greg Lopez of Examine.com insightfully noted, Fisher’s methods are quite “artsy” and particularly useful for exploratory research where theory isn’t well-established. In Fisher’s paradigm, p-values are interpreted as measures of the strength of evidence against the null hypothesis; the lower the p-value, the more strongly the result contradicts the null. Importantly, the “null” hypothesis isn’t always a “nil” hypothesis (i.e., assuming no effect), but rather a hypothesis to be “nullified.” It represents what we would expect to happen under normal circumstances, and the experiment aims to nullify this expectation. While researchers set a significance level in their minds beforehand, there isn’t a hard cutoff for decision-making.\n\n\nNeyman-Pearson, can’t we all just get along?\nFisher’s sworn enemies, Jerzey Neyman and Ergon Pearson, were highly critical of Fisher. They had another view of this whole process, and came up with this method of hypothesis testing:\n\nSet up two hypothesis, your main hypothesis (can be null) and the alternative hypothesis, and decide your \\(\\alpha\\) and \\(\\beta\\) levels before hand, which defines your rejection region\nRun the experiment, and if the data falls into the rejection region, you accept the alternative hypothesis, otherwise, accept the main hypothesis.\n\nYou are only ACTING as if it were true, not truly believing in it.\n\n\nThis seems to be much more involved and requires more prior information compared to Fisher’s approach. In this case, you technically don’t need a p-value, but instead, just the test statistic. This approach, can be very useful in fields with strong theory and/or the ability to repeat experiments or procedures over and over (think manufacturing or physics).\nIn the Neyman-Pearson approach, there is a hard cutoff determined ahead of time, to create a binary decision, acting as if a hypothesis were true. The hard cutoff requires some thinking on your end, and really depends on your scenario.\n\n\nThe Frankenstein combination of the two\nAfter learning about the Fisher and Neyman-Pearson approaches, you’ll see some similarities between the two and NHST.\n\nSet up a null hypothesis, which is always that there is “no difference” or “no zero correlation” or “zero association.\nHave an alternative hypothesis, which is always contradictory of the null.\nSet 5% as a convention for rejection the null.\nRun the experiment and report p-values as an inequality (e.g., p &lt; 0.05, p &lt; 0.001)\nDo this for every experiment forever and ever.\n\nNotice, the first step includes a null hypothesis, but the hypothesis does NOT have to be a nil hypothesis, according to the Fisher method. Additionally, this “null-hypothesis” in the first step isn’t really anywhere in the Neyman-Pearson method, which focuses on a “main hypothesis.”\n\n\nSo, what am I supposed to do?\nUnderstanding the history of how these tests were developed will allow you to be a bit more flexible in interpreting results and/or running experiments. If you’re in a field that has relatively weak theory, then perhaps you can interpret the results through the Fisher lens, and be a bit more flexible in how much the data have “nullified” the hypothesis. If you’re in a field that has a strong theory, or need to make binary (yes/no) decisions, then perhaps we can swim in the Neyman-Pearson waters for a bit, and act as if your hypothesis is true.\nBut mixing the two using NHST would not be ideal, as these two methods stem from fundamentally different philosophies, and can lead to misinterpretations of your findings or poor study design. Or worse, you can make a statistician die inside, just a tiny bit."
  },
  {
    "objectID": "posts/confidenceintervals/index.html",
    "href": "posts/confidenceintervals/index.html",
    "title": "I am 95% confident that I don’t know what a confidence interval is",
    "section": "",
    "text": "Confidence intervals (CI) have been the bane of my scientific existence. They are very unintuitive, and even statisticians debate on how to best describe a confidence interval.\nLet’s dive into the depths of my mind as I try to figure out what in the world a confidence interval is.\n\nConfident about… what?\nThe more specific definition of a 95% confidence interval (using 95% for the sake of convention) is as follows:\n\nIf I were to repeat this sampling process over and over, 95% of the confidence intervals would contain the true parameter value.\n\nNow, here’s where the confusion lies. Intuitively, this would lead you to think that it is the probability that the true value is in that interval. NOPE!\nLet’s look at it another way. Imagine we’re trying to determine the mean height of all NBA players (yes, another basketball reference). Here’s how it might play out:\n\nYou get a random sample of players, calculate the mean height, and determine a 95% confidence interval.\nI decide to do the same thing. I get my own random sample, calculate the mean, and find my own 95% confidence interval.\nThen our friend Alexis joins in. She gets her random sample and calculates her mean and 95% confidence interval.\n\nIf we (and others) kept doing this over and over, about 95% of all these confidence intervals would contain the true mean height of NBA players. However, for any single interval – yours, mine, or Alexis’s – we can’t know for certain whether it’s one of the 95% that contains the true mean or one of the 5% that doesn’t.\nI don’t think it would be very useful to demonstrate this idea in much detail, since Kristoffer Magmnusson’s visualization is more than sufficient and I would recommend playing around with it!\n\n\n\nCourtsey of Kristoffer Magmusson\n\n\nI think it’s more useful to think about how to interpret single confidence intervals. Since, a majority of the time, we’re reading studies that just spit out one confidence interval.\n\n\nInterpreting single confidence intervals\nWhen looking at a confidence interval, it’s best to think of it as a measure of how precise the estimate is. Randomness plays a huge factor in how wide a confidence interval is. However, getting larger and larger samples for your estimate can help reduce this randomness, and in turn, reduce the interval.\n\n\nMy new teaching method\nLet’s simulate a meta-analysis for the sake of example.\nIn this example, we are looking at the effect of my new teaching method called the Learning Optimization and Understanding through Interactive Engagement (LOUIE) Method, compared to traditional teaching methods, on math scores of high school students. Each study represents a different school studying the LOUIE method and comparing it to traditional teaching, and getting the mean test scores for each group.\n\n\nCode\n#load packages\nlibrary(\"metafor\")\n\n#set seed for reproducibility\nset.seed(1234)\n\n# Number of studies\nn &lt;- 5\n\n# Simulate data\nsim_data &lt;- data.frame(\n  study = paste(\"Study\", 1:n),\n  author = c(\"Smith et al.\", \"Johnson et al.\", \"Williams et al.\", \"Brown et al.\", \"Davis et al.\"),\n  year = c(2018, 2019, 2020, 2021, 2022),\n  n1i = c(100, 150, 200, 120, 250),  # sample size group 1\n  n2i = c(110, 140, 190, 130, 250),  # sample size group 2\n  m1i = c(9.2, 9.5, 9.8, 10.2, 8.4),  # mean of group 1\n  m2i = c(9.0, 9.8, 10.3, 10.9, 8.5),  # mean of group 2\n  sd1i = c(2.1, 1.9, 2.3, 2.0, 2.2),  # standard deviation of group 1\n  sd2i = c(2.2, 2.0, 2.1, 2.3, 2.1)   # standard deviation of group 2\n)\n\n#calcuate SMD\ndat &lt;- escalc(measure = \"SMD\",\n                       m1i = m1i,\n                       m2i = m2i,\n                       sd1i = sd1i,\n                       sd2i = sd2i,\n                       n1i = n1i,\n                       n2i = n2i,\n                       data = sim_data)\n\n#view\ndat\n\n\n\n    study          author year n1i n2i  m1i  m2i sd1i sd2i      yi     vi \n1 Study 1    Smith et al. 2018 100 110  9.2  9.0  2.1  2.2  0.0926 0.0191 \n2 Study 2  Johnson et al. 2019 150 140  9.5  9.8  1.9  2.0 -0.1535 0.0139 \n3 Study 3 Williams et al. 2020 200 190  9.8 10.3  2.3  2.1 -0.2263 0.0103 \n4 Study 4    Brown et al. 2021 120 130 10.2 10.9  2.0  2.3 -0.3229 0.0162 \n5 Study 5    Davis et al. 2022 250 250  8.4  8.5  2.2  2.1 -0.0464 0.0080 \n\n\nCode\n#random effects model\nmodel_result &lt;- rma(yi, vi, data = dat)\n\n#create forest plot\nforest(model_result, header = TRUE, xlab = \"LOUIE vs Traditional\")\n\n\n\n\n\n\n\n\n\nIn the meta-analysis above, we’re looking at the standardized mean differences (SMDs) of 5 studies, and their 95% confidence intervals. As we can see, maybe there’s something to the LOUIE method? All this talk about my teaching method is really a distraction, let’s look at the confidence intervals.\nFor the sake of brevity, I will not go into the math behind all of this. But in general, as the sample size increases, the confidence interval will get narrower, and thus, the estimate becomes a bit more precise.\nLet’s play around and increase the sample size of both groups in study 5 from 250 to 500. Take a look at what happens to the confidence interval of that study.\n\n\nCode\n#set seed for reproducibility\nset.seed(1234)\n\n# Number of studies\nn &lt;- 5\n\n# Simulate data\nsim_data &lt;- data.frame(\n  study = paste(\"Study\", 1:n),\n  author = c(\"Smith et al.\", \"Johnson et al.\", \"Williams et al.\", \"Brown et al.\", \"Davis et al.\"),\n  year = c(2018, 2019, 2020, 2021, 2022),\n  n1i = c(100, 150, 200, 120, 500),  # sample size group 1\n  n2i = c(110, 140, 190, 130, 500),  # sample size group 2\n  m1i = c(9.2, 9.5, 9.8, 10.2, 8.4),  # mean of group 1\n  m2i = c(9.0, 9.8, 10.3, 10.9, 8.5),  # mean of group 2\n  sd1i = c(2.1, 1.9, 2.3, 2.0, 2.2),  # standard deviation of group 1\n  sd2i = c(2.2, 2.0, 2.1, 2.3, 2.1)   # standard deviation of group 2\n)\n\n#calcuate SMD\ndat &lt;- escalc(measure = \"SMD\",\n                       m1i = m1i,\n                       m2i = m2i,\n                       sd1i = sd1i,\n                       sd2i = sd2i,\n                       n1i = n1i,\n                       n2i = n2i,\n                       data = sim_data)\n\n#view\ndat\n\n\n\n    study          author year n1i n2i  m1i  m2i sd1i sd2i      yi     vi \n1 Study 1    Smith et al. 2018 100 110  9.2  9.0  2.1  2.2  0.0926 0.0191 \n2 Study 2  Johnson et al. 2019 150 140  9.5  9.8  1.9  2.0 -0.1535 0.0139 \n3 Study 3 Williams et al. 2020 200 190  9.8 10.3  2.3  2.1 -0.2263 0.0103 \n4 Study 4    Brown et al. 2021 120 130 10.2 10.9  2.0  2.3 -0.3229 0.0162 \n5 Study 5    Davis et al. 2022 500 500  8.4  8.5  2.2  2.1 -0.0465 0.0040 \n\n\nCode\n#random effects model\nmodel_result &lt;- rma(yi, vi, data = dat)\n\n#create forest plot\nforest(model_result, header = TRUE, xlab = \"LOUIE vs Traditional\")\n\n\n\n\n\n\n\n\n\nNotice, it’s a bit more narrow. This is because, we’ve increased the sample size, thus increasing the precision of the estimate.\nTo get back to what I said earlier, this does NOT tell us how accurate this estimate is. Since we have no idea whether the studies have other sources of bias, like sampling bias, allocation bias, or measurement bias, that may affect the estimate. Increasing the sample size has increased our “confidence” about the true estimate, and that in the long run, if we do this over and over, 95% of the time, it will contain the true estimate.\n\n\nDo you feel confident?\nOverall, confidence intervals don’t make sense. But, they are useful to give us an idea of how precise an estimate is and their behavior in the long-run across identical studies. I think the confusion stems from the frequentist foundations of confidence intervals, and that our natural intuition does not think in this hypothetical world of constantly repeating experiments.\nIf you really want to get the probability that the true parameter is in the interval, then look into the Bayesian credible interval. But I can save that for another time."
  },
  {
    "objectID": "posts/ttestsarejustlinearmodels/index.html",
    "href": "posts/ttestsarejustlinearmodels/index.html",
    "title": "T-tests are just linear models",
    "section": "",
    "text": "The madness of statistical tests\nIn your introductory statistics class, you’re introduced to a myriad of statistical tests that are confusing and have weird names of scientists you’ve never heard of. You do some Googling or look in the appendix of your textbook, and you find a neat graph that looks something like this:\n\n\n\n\n\nflowchart TD\n  subgraph Parametric_assumptions[Parametric assumptions]\n        direction TB\n        Z1[1. Independent samples]\n        Z2[2. Data normally distributed]\n        Z3[3. Equal variances]\n    end\n\n    A[Type of data?] --&gt;|Continuous| B[Type of question?]\n    A --&gt;|Discrete, categorical| C[Any counts &lt; 5?]\n    \n    B --&gt;|Relationships| D[Do you have dependent & independent variables?]\n    B --&gt;|Differences| E[Differences between what?]\n    \n    C --&gt;|No| F[Chi-square tests, one and two sample]\n    C --&gt;|Yes| G[Fisher's exact test]\n    \n    D --&gt;|Yes| H[Regression analysis]\n    D --&gt;|No| I[Correlation analysis]\n    \n    I --&gt;|Parametric| J[Pearson's r]\n    I --&gt;|Nonparametric| K[Spearman's rank correlation]\n    \n    E --&gt;|Means| L[One-sample t-test]\n    E --&gt;|Variances| M[Fmax test or Bartlett's test]\n    E --&gt;|Multiple means Single variable| N[How many groups?]\n    \n    N --&gt;|Two| O[Parametric assumptions satisfied?]\n    N --&gt;|More than two| P[Parametric assumptions satisfied?]\n    \n    O --&gt;|Yes| Q[Student's t-test]\n    O --&gt;|No| R[Transform data?]\n    R --&gt;|OK| Q\n    R --&gt;|No| S[Mann-Whitney U or Wilcoxon test]\n    \n    P --&gt;|Yes| T[One-way ANOVA]\n    P --&gt;|No| U[Transform data?]\n    U --&gt;|No| V[Kruskall-Wallis test]\n    U --&gt;|OK| T\n    \n    T & V --&gt; W[If significant, do post hoc test: Bonferroni's, Dunn's, Tukey's, etc.]\n    \n    \n\n\n\n\n\n\nGreat! Now, you know exactly which test you need to do and when to do it. However, this is probably not the best statistical practice. For some if not most people, this will probably get the job done. But, you may find yourself in scenarios with messy data that do not fit neatly into the flow chart, leading you to inadvertently violate some crucial assumption of the statistical test, and your wonderfully designed study goes in the trash.\nLet me introduce you to the linear model! Believe it or not, most statistical tests are really just variations of the linear model. To understand this, let’s simulate some data and see how a simple two sample t-test is really a linear model!\n\n\nTesting the world of shoe color and dunking\nHere’s the scenario: we live in a hypothetical world where shoe color affects the vertical of basketball players. In this case, red shoes allow players to jump higher when compared to blue shoes. Since I cannot dunk a basketball (I’m 5’7”) and would love for this to be true, I’m going to put this to the test.\nI’ve recruited 80 players to study this shoe, allocating 40 players to wear the blue shoes and 40 to wear the red shoes. Each player records a vertical jump test, and their best height is recorded. Let’s simulate this data and create a box plot to see how it looks.\n\n\nCode\n#setting the parameters\nset.seed(1234)\n\nn_players &lt;- 40\nblue_mean &lt;- 28\nred_mean &lt;- 30\nsd &lt;- 3\n\n#simulate jump heights in inches\nblue_jump_height &lt;- rnorm(n_players, blue_mean, sd)\nred_jump_height &lt;- rnorm(n_players, red_mean, sd)\n\n#create a dataframe\njump_data &lt;- data.frame(\n  player_id = 1:(2 * n_players),\n  shoe_color = rep(c(\"Blue\", \"Red\"), each = n_players),\n  jump_height = c(blue_jump_height, red_jump_height)\n)\n\n#plot the data\nggplot(jump_data, aes(x = shoe_color, y = jump_height, color = shoe_color)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) + \n  geom_jitter() +\n  scale_color_manual(values = c(Blue = \"#00BFC4\", Red = \"#F8766D\")) +\n  labs(\n    x = \"Shoe color\",\n    y = \"Vertical jump (inches)\",\n  ) +\n  theme_tufte() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nIntuitively, we can tell that the blue shoes indeed increase vertical jump. However, let’s verify this with the good ole two sample t-test.\n\n\nCode\nt.test(jump_height ~ shoe_color, data = jump_data, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  jump_height by shoe_color\nt = -4.4733, df = 78, p-value = 2.586e-05\nalternative hypothesis: true difference in means between group Blue and group Red is not equal to 0\n95 percent confidence interval:\n -4.343579 -1.668066\nsample estimates:\nmean in group Blue  mean in group Red \n          26.75840           29.76422 \n\n\nNow, another way to test this is to view the problem in the context of a simple linear regression, with a binary predictor. In this case, the binary predictor would be blue shoes or red shoes. First, let’s review the formula for a simple linear regression.\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x\n\\]\nIf you’re a bit rusty:\n\n\\(\\hat{y}\\) represents is the estimated dependent variable, in this case, vertical jump height dependent on \\(x\\) (in this case, shoe color).\n\\(\\hat{\\beta}_0\\) represents the \\(\\hat{y}\\) intercept when x = 0, but in this case, it would be for those who are in the blue shoe group.\n\\(\\hat{\\beta}_1x\\) represents the estimated slope coefficient, which tell us that how much \\(y\\) (jump height) changes, on average, for a unit change in \\(x\\).\n\nWhat you’ll see in a moment, is that fitting a regression line using this binary predictor of shoe color, is the same as doing a two sample t-test.\nLet’s see what happens when we compute the linear regression.\n\n\nCode\n#the regression model\nlm(formula = jump_height ~ shoe_color, data = jump_data) |&gt; \n  summary()\n\n\n\nCall:\nlm(formula = jump_height ~ shoe_color, data = jump_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.795 -1.972 -0.425  1.714  8.489 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    26.7584     0.4751  56.317  &lt; 2e-16 ***\nshoe_colorRed   3.0058     0.6720   4.473 2.59e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.005 on 78 degrees of freedom\nMultiple R-squared:  0.2042,    Adjusted R-squared:  0.194 \nF-statistic: 20.01 on 1 and 78 DF,  p-value: 2.586e-05\n\n\nCode\n#getting the mean jump height for plotting the linear regression line\nmean_jump_data &lt;- jump_data |&gt; \n  group_by(shoe_color) |&gt; \n  summarize(mean_jump_height = mean(jump_height))\n\n#adding example regression line to previous plot\nggplot(jump_data, aes(x = shoe_color, y = jump_height, color = shoe_color)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) + \n  geom_jitter() +\n  scale_color_manual(values = c(Blue = \"#00BFC4\", Red = \"#F8766D\")) +\n  labs(\n    x = \"Shoe color\",\n    y = \"Vertical jump (inches)\",\n  ) +\n  theme_tufte() +\n  theme(legend.position = \"none\") +\n   geom_line(data = mean_jump_data, #adding example regression line\n            aes(x = shoe_color, y = mean_jump_height, group = 1),\n            linewidth = 1.2,\n            color = \"darkgray\",\n            lineend = \"round\")\n\n\n\n\n\n\n\n\n\nNow, interpreting the results of the linear regression, we can see that \\(\\hat{\\beta}_0\\) = 26.7584, in other words, the estimated average jump height for players wearing blue shoes is ≤ 26.7584 inches. Additionally, \\(\\hat{\\beta}_1x\\) = 3.0058, in other words, the average jump height of players wearing red shoes is ≤ 3.0058 inches.\n\n\nHow this all connects\nLet’s pull up the results of both the t.test and the linear regression. Can you see what the similarities are? Before moving on, take a closer look at both the results, and then scroll down.\n\n\nCode\nt.test(jump_height ~ shoe_color, data = jump_data, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  jump_height by shoe_color\nt = -4.4733, df = 78, p-value = 2.586e-05\nalternative hypothesis: true difference in means between group Blue and group Red is not equal to 0\n95 percent confidence interval:\n -4.343579 -1.668066\nsample estimates:\nmean in group Blue  mean in group Red \n          26.75840           29.76422 \n\n\nCode\nlm(formula = jump_height ~ shoe_color, data = jump_data) |&gt; \n  summary()\n\n\n\nCall:\nlm(formula = jump_height ~ shoe_color, data = jump_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.795 -1.972 -0.425  1.714  8.489 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    26.7584     0.4751  56.317  &lt; 2e-16 ***\nshoe_colorRed   3.0058     0.6720   4.473 2.59e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.005 on 78 degrees of freedom\nMultiple R-squared:  0.2042,    Adjusted R-squared:  0.194 \nF-statistic: 20.01 on 1 and 78 DF,  p-value: 2.586e-05\n\n\nWhen you dig into both results, you stumble upon some similarities:\n\nThe y-intercept: Remember \\(\\hat{\\beta}_0\\) from our regression model? It’s 26.7584. Take a peek at the mean jump height for the blue shoe group. Surprise, surprise! It’s exactly the same number.\nThe slope coefficient: Now, let’s look at \\(\\hat{\\beta}_1\\), our slope from the regression. It’s 3.0058. Here’s where it gets interesting. Grab your the means from both groups in t-test results:\n\nRed shoe group mean: 29.76422\nBlue shoe group mean: 26.75480\nDo a quick subtraction: 29.76422 - 26.75480 = 3.00582 Look familiar? It’s our regression slope!\n\n\n\n\nWhy this matters\nHopefully this example helped expand your view on statistics a bit. Viewing statistics as a bunch of inflexible rules and tests can obscure the forest for the trees, and a connection like this reveals a hidden harmony in mathematics as a whole. By understanding these connections, you might be able to start crafting solutions to problems, rather than forcing square pegs into round holes."
  },
  {
    "objectID": "posts/hypothesistesting/index.html",
    "href": "posts/hypothesistesting/index.html",
    "title": "So, hypothetically, what would we expect to happen?",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggthemes)\n\nset.seed(123)\n\n#generate data\n\nnull &lt;- rnorm(1000, mean = 0, sd = 1)\nalt &lt;- rnorm(1000, mean = 2, sd = 1)\n\n#create dataframe\ndf &lt;- data.frame(\n  null = null,\n  alt = alt\n)\n\n#create long dataframe\ndf_long &lt;- df |&gt; \n  pivot_longer(cols = everything(),\n               names_to = \"distribution\",\n               values_to = \"value\")\n\n#plot distribution\nggplot(df_long, aes(x = value, fill = distribution)) +\n  geom_density(alpha = 0.5) +\n   geom_vline(xintercept = 0, linetype = \"dashed\", color = \"#2A9D8F\", linewidth = 1) +\n  geom_vline(xintercept = 2, linetype = \"dashed\", color = \"#E76F51\", linewidth = 1) +\n  theme_tufte() +\n  theme(\n    legend.position = \"none\",\n    axis.title = element_blank()\n  ) +\n  annotate(\"text\", x = -0.6, y = 0.2, label = \"Null\\nHypothesis\", \n           color = \"#2A9D8F\", size = 3, fontface = \"bold\") +\n  annotate(\"text\", x = 2.65, y = 0.2, label = \"Alternative\\nHypothesis\", \n           color = \"#E76F51\", size = 3, fontface = \"bold\") \n\n\n\n\n\n\n\n\n\n\nNull hypothesis significance testing\nWhen you start to conduct hypothesis testing, you really start to feel like a scientist. You’re usually given this framework for testing hypotheses, called Null Hypothesis Significance Testing (NHST). It goes something like this:\n\nSet up a null hypothesis, which is always that there is “no difference” or “no correlation” or “no association.\nHave an alternative hypothesis, which is always contradictory of the null hypothesis.\nAlways set 5% as the significance level for rejecting the null hypothesis.\nRun the experiment and report p-values as an inequality if it’s significant (e.g., p &lt; 0.05).\nDo this for every experiment forever and ever.\n\nNow, this process has been largely critiqued. One of coming from the Bayesian statisticians who feel that frequentist statistics is the root of all evil. Another critique come from the more conventional/frequentist statisticians who say that it is simply a misrepresentation of how this process was initially developed. I’m a statistical agnostic, and find myself more in the latter camp in this whole debacle.\nTo understand hypothesis testing, I think it’s best to briefly learn about its history.\n\n\n\n\n\n\nNote\n\n\n\nThis blog post was inspired through a combination of this paper, Mindless Statistics by Gerd Gigerenzer (2004), and my wonderful co-worker/stoic/stats-wizard, Greg Lopez of Examine.com\n\n\n\n\nEnter, Ronald Fisher\nThe statistician Ronald Fisher first came up with this idea of p-values back in the 1920’s, and came up with a whole process for testing hypotheses in the 1950s, which is as follows:\n\nSet up a null hypothesis (this does not have to be a nil hypothesis, that there’s no difference or no association).\nDetermine your level of significance (it does not have to be 5%)\nRun the experiment and report the exact level of significance/p-value (e.g., p = 0.0412, NOT p &lt; 0.05)\nIf reaches the significance level you determined, you reject the null hypothesis. If it doesn’t, you fail to reject the null hypothesis.\n\nThis looks a bit different than what we first outlined from NHST. In Fisher’s world, there is no “alternative” hypothesis and the concept of a fixed significance level, such as 5%, was more of a recommendation than a strict rule. As my former coworker Greg Lopez of Examine.com insight-fully noted, Fisher’s methods are quite “artsy” and particularly useful for exploratory research where theory isn’t well-established.\nIn Fisher’s paradigm, p-values are interpreted as measures of the strength of evidence against the null hypothesis; the lower the p-value, the more strongly the result contradicts the null. Importantly, the “null” hypothesis isn’t always a “nil” hypothesis (i.e., assuming no effect), but rather a hypothesis to be “nullified.” It represents what we would expect to happen under a specific circumstance, and the experiment aims to nullify this expectation. While researchers set a significance level beforehand, there isn’t a hard cutoff like in NHST. However, some other folks disagreed.\n\n\nNeyman-Pearson, can’t we all just get along?\nFisher’s sworn enemies, Jerzey Neyman and Ergon Pearson, had another view of this whole process. They came up with this method of hypothesis testing:\n\nSet up two hypotheses, your main hypothesis (usually a nil hypothesis) and the alternative hypothesis (usually contradictory of the null).\nDecide your \\(\\alpha\\) (false positive) and \\(\\beta\\) (false negative) levels before hand, which defines your rejection region.\nRun the experiment, and if the data falls into the rejection region, you accept the alternative hypothesis, otherwise, accept the main hypothesis.\n\nYou are only ACTING as if the hypothesis were true, not truly believing in it.\n\n\nThis seems to be much more involved and requires more prior information compared to Fisher’s approach. In this case, you technically don’t need a p-value, but instead, just the test statistic. This approach, can be very useful in fields with strong theory and/or the ability to repeat experiments or procedures over and over (think manufacturing or physics).\nIn the Neyman-Pearson approach, there is a hard cutoff determined ahead of time, to create a binary decision, allowing you to act as if a hypothesis were true. The hard cutoff requires some thinking on your end and really depends on how rigorous you want to be.\n\n\nThe Frankenstein combination of the two\nAfter learning about the Fisher and Neyman-Pearson approaches, you’ll see that NHST is a weird combination of the two. Let’s look at NHST again:\n\nSet up a null hypothesis, which is always that there is “no difference” or “no correlation” or “no association.”\nHave an alternative hypothesis, which is always contradictory of the null.\nSet 5% as a convention for rejection the null.\nRun the experiment and report p-values as an inequality (e.g., p &lt; 0.05, p &lt; 0.001)\nDo this for every experiment forever and ever.\n\nNotice, the first step includes a null hypothesis, but the hypothesis does NOT have to be a nil hypothesis, but in NHST, it always has to be a nil hypothesis. Additionally, this “null-hypothesis” in the first step isn’t really anywhere in the Neyman-Pearson method, which focuses on a “main hypothesis.” Do you see any other parts where NHST overlaps with Fisher and Neyman-Pearson’s methods?\n\n\nSo, what am I supposed to do?\nUnderstanding the history of how these tests were developed will allow you to be a bit more flexible in interpreting results and/or running experiments. If you’re in a field that has relatively weak theory, then perhaps you can interpret results through the Fisher lens, and be a bit more flexible in how much the data have “nullified” the hypothesis. If you’re in a field that has a strong theory, or need to make binary (yes/no) decisions, then perhaps we can swim in the Neyman-Pearson waters for a bit, and act as if your hypothesis is true.\nBut mixing the two using NHST would not be ideal, as these two methods stem from fundamentally different philosophies, and can lead to misinterpretations of your findings or poor study design. Or worse, you can make a statistician die inside, just a tiny bit."
  }
]